{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Linear Regression\n",
    "\n",
    "## Linear Models\n",
    "In simple words a linear regression is a methods to find the best linear model for a set of data.\n",
    "\n",
    "What is a linear model?\n",
    "\n",
    "A linear model is a mathematical function that makes predictions using a linear combination of its input x.\n",
    "Mathematically, it looks like this:\n",
    "\n",
    "$$f(x; w, b) = w^T x + b = w_1 x_1 + ... + w_D x_D + b$$\n",
    "\n",
    "where:\n",
    "* $\n",
    "\\mathbf{x} = \\begin{pmatrix}\n",
    "x_1 \\\\\n",
    "x_2 \\\\\n",
    "\\vdots \\\\\n",
    "x_n\n",
    "\\end{pmatrix} \n",
    "\\in R^D$ \n",
    "is vector of D real features reprensenting the input data.\n",
    "\n",
    "* w,b are the parameters of the linear model:\n",
    "    * $\n",
    "    \\mathbf{w} = \\begin{pmatrix}\n",
    "    w_1 \\\\\n",
    "    w_2 \\\\\n",
    "    \\vdots \\\\\n",
    "    w_n\n",
    "\\end{pmatrix}$ is a vector D parameters\n",
    "    *  b a single parameter (a scalar) often called the bias parameters. \n",
    "\n",
    "\n",
    "## A concrete Example to better understand: \n",
    "\n",
    "Imagine our task was to predict the strength of concrete batch based on the following features:\n",
    "    * $x_1$: the amount of water (in mL) \n",
    "    * $x_2$: the amount of sand (in kg)  \n",
    "If we believe the relation between these features and the strength of the concrete to be linear we could use a linear/\n",
    "\n",
    "Then, based on the our linear model would say the strength of the resulting concrete would be: \n",
    "\n",
    "$$f(x1, x2; w, b) = w_1 x_1 + w_2  x_2 +b $$\n",
    "\n",
    "If we set the parameters to $w_1 = w_2 = 1$ and $b = 0$: <br>\n",
    "For $x_1 = 100 ml$ and $x_2 = 1kg$ this would yield $f(x_1, x2; w, b) = 100 + 1 = 101 N$ \n",
    "\n",
    "\n",
    "However, it is highly that the parameters we chosed are the correct ones. If we where experts in making we would likely have an idea on how to set these parameters to get the correct result. \n",
    "\n",
    "But if it is not the case, the linear regression finds the paremeters w and b that give the best model for a given set of data. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is an example in 2 dimension (i.e 1 input features and an output dimenssions of size 1.) You can try changing the value of the parameters to see how they influence the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e45871eadba14bb492d7c715db88cc5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=0.0, description='w', max=2.0, min=-2.0), FloatSlider(value=0.0, descr…"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from ipywidgets import interactive\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "plt.figure(2)\n",
    "x = np.linspace(-100, 100, num=1000)\n",
    "\n",
    "def f(w, b):\n",
    "    plt.plot(x, w * x + b)\n",
    "    plt.ylim(-5, 5)\n",
    "    plt.xlim(-10, 10)\n",
    "    plt.xlabel(\"x\")\n",
    "    plt.ylabel(\"f(x;w,b)\")\n",
    "    plt.show()\n",
    "\n",
    "interactive_plot = interactive(f, w=(-2.0, 2.0), b=(-3, 3, 0.5))\n",
    "output = interactive_plot.children[-1]\n",
    "output.layout.height = '450px'\n",
    "interactive_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do we find the best value for w and b?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As explain the introduction we need example to learn from:\n",
    "\n",
    "Let the $\\{x^{(n)}, y^{(n)}\\}_{n=1}^{N_{train}}$ our training dataset (i.e. the set set of data we want to learn from).\n",
    "\n",
    "For example we have N batchs of ciment each with a different set of characteristics $x^{(n)} \\in R^D$ and a corresponding output strengh $y^{(n)}$.\n",
    "\n",
    "How can we find the linear model that best fits this data.\n",
    "\n",
    "Assuming the training was generated using a linear model f(x, w, b) to which we added noise. the resulting output is:\n",
    "\n",
    "$$y^{n} = f(x^{(n)}, w, b) + y^{(n)}_{noise}$$\n",
    "\n",
    "We have generated a set of $N_{train} = 20$ example this way. \n",
    "Try changing the parameter the parameter w,b to fit the points once you are ready you can check reveal and the true plot will be displayed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05c29084ef5342cba9ab56bbe494ea5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=0.0, description='w', max=2.0, min=-2.0), FloatSlider(value=0.0, descr…"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interactive\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "N_train = 20\n",
    "x = np.linspace(-10, 10, num=1000)\n",
    "x_train = np.linspace(-10, 10, num= N_train)\n",
    "sigma_noise = 1\n",
    "w_true =  0.345\n",
    "b_true = 1\n",
    "y_noise = sigma_noise *np.random.randn(N_train)\n",
    "def f(w, b, reveal):\n",
    "    plt.plot(x_train, w_true * x_train + b_true + y_noise, 'x')\n",
    "    plt.plot(x, w * x + b)\n",
    "    if reveal: \n",
    "        plt.plot(x, w_true * x + b_true)\n",
    "    plt.ylim(-5, 5)\n",
    "    plt.xlabel(\"x\")\n",
    "    plt.ylabel(\"f(x;w,b)\")\n",
    "    plt.show()\n",
    "\n",
    "interactive_plot: interactive = interactive(f, w=(-2.0, 2.0), b=(-3, 3, 0.5), reveal = False)\n",
    "\n",
    "\n",
    "output = interactive_plot.children[-1]\n",
    "output.layout.height = '450px'\n",
    "\n",
    "interactive_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What you have likely done intuitively is minimizing the distance between each points and there corresponding projection on the linear model outputs. Mathematically this can be formalized as follow: \n",
    "\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(w, b) = \\frac{1}{N} \\sum_{n=1}^N \\left( f(x^{(n)}; w, b) - y^{(n)} \\right)^2\n",
    "$$\n",
    "\n",
    "This is called the mean sqare error as the mean of the square error of the model on each training input-output pairs.\n",
    "\n",
    "Then finding the best parameters (w^*, b^*) can be written: \n",
    "\n",
    "$$\n",
    "(w^*, b^*) = \\underset{w, b}{\\text{argmin}} \\; \\mathcal{L}(w, b)\n",
    "$$\n",
    "\n",
    "To solve this in a program we first need to stack: \n",
    "* the training input features $x^{(n)}$ into a single matrix: $X  = \\begin{pmatrix}\n",
    "x^{(1)} \\\\\n",
    "x^{(2)}\\\\\n",
    "\\vdots \\\\\n",
    "x^{(N_{train})}\n",
    "\\end{pmatrix} = \\begin{pmatrix}\n",
    "x^{(1)}_1 & \\dots & x^{(1)}_D\\\\\n",
    "x^{(2)}_1 & \\dots & x^{(2)}_D\\\\\n",
    "& \\vdots & \\\\\n",
    "x^{(N_{train})}_1 & \\dots & x^{(N_{train})}_D\n",
    "\\end{pmatrix}$\n",
    "* the traing ouputs $y^{(n)}$ into a single vector:  $\n",
    "\\mathbf{y} = \\begin{pmatrix}\n",
    "y^{(1)} \\\\\n",
    "y^{(2)} \\\\\n",
    "\\vdots \\\\\n",
    "y^{(N_{train})}\n",
    "\\end{pmatrix}$\n",
    "\n",
    "This is because programs are optimized for array and matrices.\n",
    "\n",
    "We can then write the linear model with the resulting input matrix $X$ and output vecotr $y$: \n",
    "\n",
    "$$\\hat{y} = \\begin{pmatrix}\n",
    "f(x^{(1)}; w,b) \\\\\n",
    "f(x^{(2)}; w,b) \\\\\n",
    "\\vdots \\\\\n",
    "f(x^{(N_{train})}; w,b)\n",
    "\\end{pmatrix} = \\begin{pmatrix}\n",
    "w^T x^{(1)} + b \\\\\n",
    "w^T x^{(2)} + b \\\\\n",
    "\\vdots \\\\\n",
    "w^T x^{(N_{train})} + b\n",
    "\\end{pmatrix} = X w + b$$\n",
    "\n",
    "Strictly speaking we should write  $b 1_D = b \\begin{pmatrix} \n",
    "1 \\\\\n",
    "1 \\\\\n",
    "\\vdots \\\\\n",
    "1\n",
    "\\end{pmatrix}$ instead of just b in the left hand side of the last equality.\n",
    "\n",
    "But this is often written as just b with the implicit idea that $b$ is added to every line of $Xw$.\n",
    "\n",
    "One can further simplify the notation removng b from the linear model entirely by notice that: \n",
    "$$\n",
    "X w + b = \\begin{pmatrix}\n",
    "x^{(1)}_1 & \\dots & x^{(1)}_D & 1\\\\\n",
    "x^{(2)}_1 & \\dots & x^{(2)}_D & 1\\\\\n",
    "& \\vdots & &1\\\\\n",
    "x^{(N_{train})}_1 & \\dots & x^{(N_{train})}_D & 1\n",
    "\\end{pmatrix} \\begin{pmatrix}\n",
    "w^{(1)} \\\\\n",
    "w^{(2)} \\\\\n",
    "\\vdots \\\\\n",
    "w^{D}\\\\\n",
    "b\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "In the following we won't refer to b has it will be implicitely part of w.\n",
    "\n",
    "The loss funtction can be rewritten with the matrix $X$ and vectors $y$, $\\hat{y}$:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(w, b) = (y-Xw)^T (y-Xw)\n",
    "$$\n",
    "\n",
    "Check you can see why (don't forget that b in w now). \n",
    "\n",
    "### Reminder: \n",
    "\n",
    "In 1D the minum and maximum of a convex function $g:\n",
    "\\mathbb{R}\n",
    "\\rightarrow\n",
    "\\mathbb{R}\n",
    "$ are the points x such that (s.t.):\n",
    "\n",
    "$g'(x) = \n",
    "\\frac{dg}{dx}\n",
    "= 0$\n",
    "\n",
    "In general case of D dimension, for a convex function $g: \\mathbb{R}^D\n",
    "\\rightarrow\n",
    "\\mathbb{R}$, the minimums $x \\in \\mathbb{R}^D$ are points such that\n",
    "\n",
    "$$\n",
    "\\nabla g(x) = \\begin{pmatrix}\n",
    "\\frac{\\partial g(x)}{\\partial x_1}\\\\\n",
    "\\frac{\\partial g(x)}{\\partial x_2}\\\\\n",
    "\\vdots \\\\\n",
    "\\frac{\\partial g(x)}{\\partial x_D}\\\\\n",
    "\\end{pmatrix} = \\begin{pmatrix}\n",
    "0\\\\\n",
    "\\vdots \\\\\n",
    "0\\\\\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "In our case we can show that: \n",
    "\n",
    "$$\n",
    "\\nabla_w\\mathcal{L}(w) = X^T(y-XW)\n",
    "$$\n",
    "\n",
    "Therefore the solution to the problem is a solution of : \n",
    "\n",
    "$$\n",
    " X^T(y-XW) = 0\n",
    "$$\n",
    "\n",
    "Assuming $X^T X$  is non-singular: \n",
    "\n",
    "w^* = (X^T X)^-1 X^T y\n",
    "\n",
    "This a classic maths problem and so function have already been implemeted in python: \n",
    "\n",
    "```python\n",
    "w_fit = np.linalg.lstsq(X, yy, rcond=None)[0]\n",
    "```\n",
    "\n",
    "## Generalized Linear Regression Model:\n",
    "\n",
    "\n",
    "The linear assumption we have been making since the bigining is often not true. \n",
    "If we modify the input: we obtain a new set of features: \n",
    "\n",
    "$\\tilde{x} =  \\phi(x)$\n",
    "\n",
    "This weekens the linear assumption into the following assumption: \n",
    "\n",
    "The output is a linear function of the parameters.\n",
    "\n",
    "Common functions used to modify the input data include:\n",
    "\n",
    "* Radial Basis function (RBF):\n",
    "\n",
    "$$\n",
    "\\phi_{RBF}(x; c, \\sigma) = \\exp\\left(-\\frac{\\|x - c\\|^2}{2\\sigma^2}\\right)\n",
    "$$\n",
    "\n",
    "* logistic-Sigmoid:\n",
    "$$\n",
    "\\sigma(x; v, b) = \\frac{1}{1 + e^{-v^T x -b}}\n",
    "$$\n",
    "\n",
    "Bellow are representations of the function you can try to play with the parameters to get intuition into their roles:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5caefe4ee3b4ce4a00dc2990624d404",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=1.0, description='σ (RBF)', max=5.0, min=0.1), FloatSlider(value=0.0, …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What to do next:\n",
    "\n",
    "This small course on linear regression is just a reformulation of the machine learning and pattern recognition (MLPR) of Ian Murray: https://mlpr.inf.ed.ac.uk/2022/notes/w0a_welcome_and_advice.html <br>\n",
    "\n",
    "I would advise to read the follwing sections:\n",
    "-  The linear regression\n",
    "- gradient descent\n",
    "- Train, Test split\n",
    "- logistic sigmoid\n",
    "- neural networks\n",
    "\n",
    "You can read the other sections but they might not be usefull for this internship."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
